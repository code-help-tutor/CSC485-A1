# University of Toronto CSC 485H/2501H: Computational linguistics Assignment 1

**Due date**: 17:00 on Thursday, October 3, 2024.

This assignment consists of two main parts: transition-based dependency parsing and graph-based dependency parsing.

## 1. Transition-based dependency parsing (42 marks)

### (a) Completing the sequence of transitions
For parsing the sentence “To raise those doubts is to resolve them” with the given dependency tree, the sequence of transitions is as follows:

| Step | Stack | Buffer | New dep | Transition |
| ---- | ----- | ------ | ------- | ---------- |
| 0    | [ROOT] | [To, raise, those, doubts, is, to, resolve, them] | raise mark To → | SHIFT |
| 1    | [ROOT, To] | [raise, those, doubts, is, to, resolve, them] |  | SHIFT |
| 2    | [ROOT, To, raise] | [those, doubts, is, to, resolve, them] |  | LEFT-ARC |
| 3    | [ROOT, raise] | [those, doubts, is, to, resolve, them] |  | SHIFT |
| 4    | [ROOT, raise, those] | [doubts, is, to, resolve, them] |  | SHIFT |
| 5    | [ROOT, raise, those, doubts] | [is, to, resolve, them] | doubts mark those → | LEFT-ARC |
| 6    | [ROOT, raise, doubts] | [is, to, resolve, them] |  | SHIFT |
| 7    | [ROOT, raise, doubts, is] | [to, resolve, them] | is mark raise → | LEFT-ARC |
| 8    | [ROOT, raise, is] | [to, resolve, them] |  | SHIFT |
| 9    | [ROOT, raise, is, to] | [resolve, them] | to mark is → | LEFT-ARC |
| 10   | [ROOT, raise, to] | [resolve, them] |  | SHIFT |
| 11   | [ROOT, raise, to, resolve] | [them] | resolve mark to → | LEFT-ARC |
| 12   | [ROOT, raise, resolve] | [them] |  | SHIFT |
| 13   | [ROOT, raise, resolve, them] | [] | them mark resolve → | RIGHT-ARC |
| 14   | [ROOT, raise, resolve] | [] | resolve mark raise → | RIGHT-ARC |
| 15   | [ROOT, raise] | [] | raise mark ROOT → | RIGHT-ARC |

### (b) Number of steps for parsing a sentence
A sentence containing n words will be parsed in 2n - 1 steps. This is because initially there are n words in the buffer and ROOT in the stack. At each SHIFT operation, one word is moved from the buffer to the stack. This happens n times until the buffer is empty. Then, for each non-ROOT word on the stack, there is one LEFT-ARC or RIGHT-ARC operation to establish its dependency, which is n - 1 operations. So in total, there are n + (n - 1) = 2n - 1 steps.

### (c) Implementing methods in PartialParse class
In `q1_parse.py`, the `complete` and `parse_step` methods in the `PartialParse` class need to be implemented. These methods implement the transition mechanism of the parser. Also, `get_nrightmost` and `get_nleftmost` need to be implemented. Running `python3 run_test.py q1-c` can perform basic tests.

### (d) Implementing minibatch parsing algorithm
In `q1_parse.py`, the `minibatch_parse` function needs to be implemented according to algorithm 1. This function parses sentences in minibatches for more efficient neural network prediction. Running `python3 run_test.py q1-d` can perform basic tests.

### (e) Implementing the oracle
In `q1_parse.py`, the `get_oracle` method needs to be implemented. This oracle provides the next transition to take for a partial parse given a set of correct, final target dependency arcs. Running `python3 run_test.py q1-e` can perform basic tests.

### (f) Implementing neural network classifier
In `q1_model.py`, the neural network classifier governing the dependency parser needs to be implemented by filling in the appropriate sections marked by "ENTER YOUR CODE BELOW". This includes constructing and training a neural network to predict the next transition to apply given the state of the stack, buffer, and dependencies. Running `python3 train.py q1` trains and evaluates the model on a corpus of English Web text annotated with Universal Dependencies. With correct implementation, an LAS of around 80% on both the validation and test sets can be achieved.

### (g) Bonus
Explore whether ChatGPT can perform dependency parsing in a zero-shot or few-shot manner. Compare the dependency parse of your model and ChatGPT using the sentence “To ask those questions is to answer them.” Report the prompt used for the experiment and qualitatively evaluate the performance of ChatGPT and your own model, noting any advantages and disadvantages.

## 2. Graph-based dependency parsing (58 marks)

### (a) Limitation of transition-based parsing mechanism
The parsing mechanism described above is insufficient to generate non-projective dependency trees because it is limited to only being able to parse projective dependency trees. In a projective dependency tree, the edges can be drawn above the words without crossing other edges when the words, preceded by ROOT, are arranged in linear order. Equivalently, every word forms a contiguous substring of the sentence when taken together with its descendants. The transition-based mechanism works by incrementally building a parse state represented by a stack, buffer, and list of dependencies. It can only handle cases where the edges can be drawn without crossing, which is the definition of a projective tree.

### (b) Implementing is_projective function
In `q2_algorithm.py`, the `is_projective` function needs to be implemented. Running `python3 q2_algorithm.py` and `python3 run_test.py q2` can perform basic tests and report the result.

### (c) Computing gap degree
For the first tree (rooted at “Sam met a student today who has a linguistics degree”):
- The subsequence for “Sam” and its descendants is “Sam met a student today who has a linguistics degree”. This is one contiguous substring, so the gap degree for “Sam” is 0.
- For “a student”, the subsequence is “a student who has a linguistics degree”. This is also one contiguous substring, so the gap degree for “a student” is 0.
- For “today”, the subsequence is “today”. It is a single word, so the gap degree is 0.
- For “who has”, the subsequence is “who has a linguistics degree”. Again, one contiguous substring, gap degree is 0.
- For “a linguistics degree”, the subsequence is “a linguistics degree”. Gap degree is 0.
So, the gap degree of this tree is 0.

For the second tree (shown in the assignment description):
- Consider “ROOT”. Since it has no descendants in this context (for gap degree calculation), its gap degree is 0.
- For each of the other words, it's a bit more complex to determine as the tree is not fully provided in this description. However, without a detailed view of the tree structure, it's impossible to accurately calculate the gap degree for each word and hence the overall gap degree of the tree.

### (d) Implementing arc scorer
In `q2_model.py`, implement `create_arc_layers` and `score_arcs`. Determine the appropriate dimensions for `WA` and `bA`, and implement the arc score calculations for the batched sentence tensors using PyTorch tensor operations. Avoid using loops for this question to avoid a penalty.

### (e) Initialization of weight matrices
For effective, stable training, especially for deeper networks, it is important to initialize weight matrices carefully. A standard initialization strategy for ReLU layers is Kaiming initialization. For a given layer's weight matrix W of dimension m × n, where m is the number of input units to the layer and n is the number of units in the layer, Kaiming initialization samples values Wi j from a Gaussian distribution with mean μ = 0 and variance σ² = 2/m. However, it is common to instead sample from a uniform distribution U(a, b). To derive the values of a and b that yield a uniform distribution with the mean and variance given above:
- The mean of a uniform distribution U(a, b) is (a + b) / 2. Setting this equal to 0 (mean of the Gaussian distribution), we get a + b = 0, so b = -a.
- The variance of a uniform distribution U(a, b) is (b - a)² / 12. Substituting b = -a and setting this equal to σ² = 2/m, we get (-a - a)² / 12 = 2/m. Simplifying, 4a² / 12 = 2/m, which gives a² = 6/m. Thus, a = sqrt(6/m) and b = -sqrt(6/m).

### (f) Implementing label scorer
In `q2_model.py`, implement `create_label_layers` and `score_labels`. Determine the appropriate dimensions for the trainable parameters and implement the label score calculations for the batched sentence tensors using PyTorch tensor operations. Avoid using loops to avoid a penalty.

### (g) Inclusion of terms in arc and label scorers
In the arc scorer, the score function includes a term that has `HA` but not `DA`, but there isn't a term that has the opposite inclusions (DA but not HA). This is because the arc scorer is designed to score the dependency edges from one vertex to another. The term with `HA` represents the contribution of the head vertex, and there is no need for a term with only `DA` as it doesn't provide additional useful information for scoring the arc. In the label scorer, both are included because the label scorer is more complex and needs to consider both the head and the dependent vertices in different ways to score the possible dependency relations.

### (h) Multiple multiplications in this model
In standard classification problems, we typically multiply the input by a weight matrix and add a per-class bias to produce a class score for the given input. In this case, we have to multiply `WA` and `WL` by (transformed versions of) the input twice because the model is designed to capture the complex relationships between the vertices in the dependency graph. The double multiplication allows for a more expressive model that can better learn the arc and label scores by considering different combinations of the input features.

### (i) Implementing constraints
In `q2_model.py`, implement the `mask_possible` function to enforce constraints on which arcs and arc-label combinations are possible.

### (j) Using maximum spanning tree algorithm
We need to use a maximum spanning tree algorithm to make the final arc prediction because in a dependency graph, we are looking for a set of arcs that form a tree structure and maximize the overall score. We can't just use argmax like in a typical classification scenario because that would not guarantee a tree structure. If we use argmax instead, we would likely end up with a set of edges that are not a valid tree, which would not represent a correct dependency parse.

### (k) Implementing remaining functions
In `q2_algorithm.py`, finish the implementation of `is_single_root` and `mst_single_root`. Refer to the pseudocode in the third edition of the Jurafsky & Martin textbook for guidance.

### (l) Finding example sentences and comparing parses
Find at least three example sentences where the transition-based parser gives a different parse than the graph-based parser, but both parses are wrong as judged by the annotation in the corpus. For each example, argue for which of the two parses you prefer.

**Submission Instructions**:
This assignment is submitted electronically via MarkUs. Submit a total of seven required files:
- `a1written.pdf`: PDF document containing written answers.
- `q1_model.py`: With implementations filled in.
- `q1_parse.py`: With implementations filled in.
- `q2_algorithm.py`: With implementations filled in.
- `q2_model.py`: With implementations filled in.
- `weights-q1.pt` and `weights-q2.pt`: Weights files produced by models' final runs.

这是多伦多大学计算机科学系 2024 年秋季课程《计算语言学》的作业 1。作业主要分为两个部分：过渡式依存句法分析（42 分）和基于图的依存句法分析（58 分）。

**一、过渡式依存句法分析**
1. **完成转换序列**：给定句子“To raise those doubts is to resolve them”和对应的依存树，完成该句子的转换序列，需指出每一步的栈、缓冲区状态以及应用的转换和添加的依存关系。
2. **计算解析步数**：说明包含 n 个单词的句子在这种解析方式下需要多少步，并简要解释原因。
3. **实现 PartialParse 类的方法**：在`q1_parse.py`文件中实现`PartialParse`类的`complete`和`parse_step`方法，以及`get_nrightmost`和`get_nleftmost`方法。可通过运行`python3 run_test.py q1-c`进行基本测试。
4. **实现小批次解析算法**：在`q1_parse.py`中实现小批次解析算法，按照给定算法进行句子的小批次解析。可通过运行`python3 run_test.py q1-d`进行测试。
5. **实现神谕**：在`q1_parse.py`中实现`get_oracle`方法，该方法根据部分解析和正确的最终目标依存弧集提供下一步要采取的转换。可通过运行`python3 run_test.py q1-e`进行测试。
6. **实现神经网络分类器**：在`q1_model.py`中实现神经网络分类器，通过填充相应部分构建和训练一个神经网络，用于预测给定栈、缓冲区和依存关系状态下的下一步转换。运行`python3 train.py q1`可在标注的语料库上训练和评估模型，正确实现时在验证集和测试集上应能达到约 80%的 LAS。
7. **Bonus**：探索 ChatGPT 是否能进行零样本或少样本的依存句法分析，比较自己模型和 ChatGPT 对特定句子的解析结果，报告实验中使用的提示语，并定性评估两个模型的性能及优缺点。

**二、基于图的依存句法分析**
1. **说明过渡式解析机制的局限性**：解释为什么上述过渡式解析机制不足以生成非投射性依存树。
2. **实现 is_projective 函数**：在`q2_algorithm.py`中实现`is_projective`函数，可通过运行`python3 q2_algorithm.py`和`python3 run_test.py q2`进行测试。
3. **计算间隙度**：计算两个给定依存树中每个词的间隙度以及整个树的间隙度。
4. **实现弧评分器**：在`q2_model.py`中实现`create_arc_layers`和`score_arcs`，确定适当的维度并使用 PyTorch 张量操作进行弧评分计算，避免使用循环以免被扣分。
5. **初始化权重矩阵**：推导用于 ReLU 层的标准初始化策略（Kaiming 初始化）在使用均匀分布时的上下界值。
6. **实现标签评分器**：在`q2_model.py`中实现`create_label_layers`和`score_labels`，确定适当的维度并使用 PyTorch 张量操作进行标签评分计算，避免使用循环以免被扣分。
7. **解释弧和标签评分器中的项**：说明为什么在弧评分器中有包含`HA`但没有包含`DA`的项，而在标签评分器中两者都有。
8. **解释多次乘法的原因**：解释为什么在这个模型中需要对`WA`和`WL`进行两次与输入的乘法。
9. **实现约束**：在`q2_model.py`中实现`mask_possible`函数以实施对可能的弧和弧标签组合的约束。
10. **解释使用最大生成树算法的原因**：说明为什么在最终弧预测中需要使用最大生成树算法，而不能像典型分类场景那样仅使用 argmax，以及如果使用 argmax 会发生什么。
11. **实现剩余函数**：在`q2_algorithm.py`中完成`is_single_root`和`mst_single_root`的实现，可参考 Jurafsky & Martin 教科书的伪代码。
12. **找例子并比较解析**：找到至少三个过渡式解析器和基于图的解析器给出不同解析且都与语料库标注不同的句子示例，并对两个解析进行比较选择偏好。

**提交说明**：通过 MarkUs 电子提交七个文件，包括 PDF 文档、填充了实现的代码文件以及模型权重文件等。同时提供了调试和训练的一些注意事项，如使用特定标志运行代码进行快速调试、在不同环境下的训练时间、使用 GPU 训练等。还介绍了 PyTorch 的相关内容及安装要求，它是用于实现神经依存解析器组件的库，提供自动微分等功能。
